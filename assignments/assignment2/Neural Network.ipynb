{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_svhn, random_split_train_val\n",
    "from gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from layers import FullyConnectedLayer, ReLULayer\n",
    "from model import TwoLayerNet\n",
    "from trainer import Trainer, Dataset\n",
    "from optim import SGD, MomentumSGD\n",
    "from metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient check passed!\n",
      "Gradient check passed!\n",
      "Gradient check passed!\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for (0, 'W')\n",
      "Gradient check passed!\n",
      "Checking gradient for (0, 'B')\n",
      "Gradient check passed!\n",
      "Checking gradient for (2, 'W')\n",
      "Gradient check passed!\n",
      "Checking gradient for (2, 'B')\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums_train = train_X.shape[0]\n",
    "shuffled_indices = np.arange(nums_train)\n",
    "np.random.shuffle(shuffled_indices)\n",
    "train_len = int(0.7 * (nums_train))\n",
    "X_train = train_X[shuffled_indices[:train_len]]\n",
    "y_train = train_y[shuffled_indices[:train_len]]\n",
    "X_val = train_X[shuffled_indices[train_len:]]\n",
    "y_val = train_y[shuffled_indices[train_len:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking gradient for (0, 'W')\n",
      "Gradient check passed!\n",
      "Checking gradient for (0, 'B')\n",
      "Gradient check passed!\n",
      "Checking gradient for (2, 'W')\n",
      "Gradient check passed!\n",
      "Checking gradient for (2, 'B')\n",
      "Gradient check passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_reg.predict(train_X[:30])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-2)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(train_history)\n",
    "# plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "# model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "# dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "# trainer = Trainer(model, dataset, SGD(), learning_rate_decay=0.99)\n",
    "\n",
    "# initial_learning_rate = trainer.learning_rate\n",
    "# loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "# assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "# assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 42.241475, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 44.886053, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 42.114705, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 43.362908, Train accuracy: 0.196667, val accuracy: 0.206000\n",
      "Loss: 38.689303, Train accuracy: 0.206333, val accuracy: 0.219000\n",
      "Loss: 39.848849, Train accuracy: 0.250556, val accuracy: 0.252000\n",
      "Loss: 43.589987, Train accuracy: 0.275667, val accuracy: 0.277000\n",
      "Loss: 41.658010, Train accuracy: 0.290778, val accuracy: 0.285000\n",
      "Loss: 40.675572, Train accuracy: 0.345444, val accuracy: 0.349000\n",
      "Loss: 38.559594, Train accuracy: 0.399667, val accuracy: 0.390000\n",
      "Loss: 35.180066, Train accuracy: 0.433556, val accuracy: 0.428000\n",
      "Loss: 29.114179, Train accuracy: 0.470444, val accuracy: 0.462000\n",
      "Loss: 31.537373, Train accuracy: 0.498778, val accuracy: 0.499000\n",
      "Loss: 32.718789, Train accuracy: 0.520000, val accuracy: 0.527000\n",
      "Loss: 33.333823, Train accuracy: 0.548667, val accuracy: 0.548000\n",
      "Loss: 33.692586, Train accuracy: 0.558667, val accuracy: 0.559000\n",
      "Loss: 29.000826, Train accuracy: 0.577778, val accuracy: 0.570000\n",
      "Loss: 34.493050, Train accuracy: 0.610111, val accuracy: 0.587000\n",
      "Loss: 30.329043, Train accuracy: 0.616111, val accuracy: 0.604000\n",
      "Loss: 30.241384, Train accuracy: 0.630000, val accuracy: 0.627000\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), learning_rate=1e-4, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 11.449705, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 11.567443, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 12.760513, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 12.086189, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 20.466829, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 25.126151, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 43.138005, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 34.129820, Train accuracy: 0.200000, val accuracy: 0.000000\n",
      "Loss: 37.706869, Train accuracy: 0.200000, val accuracy: 0.200000\n",
      "Loss: 354.156138, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: 1113.769120, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: 1116.027848, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.200000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ildarnasrtdinov/Work/education/dlcourse/assignments/assignment2/layers.py:42: RuntimeWarning: divide by zero encountered in log\n",
      "  ans = -np.sum(np.log(probs[np.arange(len(target_index)), target_index.reshape(1, -1)]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.266667\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.266667\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.400000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.333333\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.333333\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.400000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.400000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.466667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.266667\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.333333\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.266667\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.000000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.066667\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.266667, val accuracy: 0.066667\n"
     ]
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 11.864046, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 11.128917, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 9.940287, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: 9.825753, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 13.605160, Train accuracy: 0.266667, val accuracy: 0.133333\n",
      "Loss: 9.744032, Train accuracy: 0.333333, val accuracy: 0.133333\n",
      "Loss: 12.872586, Train accuracy: 0.266667, val accuracy: 0.066667\n",
      "Loss: 15.841548, Train accuracy: 0.333333, val accuracy: 0.066667\n",
      "Loss: 10.804288, Train accuracy: 0.533333, val accuracy: 0.066667\n",
      "Loss: 14.496527, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 9.089495, Train accuracy: 0.333333, val accuracy: 0.133333\n",
      "Loss: 284.363480, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: 1678.967547, Train accuracy: 0.200000, val accuracy: 0.066667\n",
      "Loss: 1180.122249, Train accuracy: 0.400000, val accuracy: 0.000000\n",
      "Loss: inf, Train accuracy: 0.400000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.200000, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.133333\n",
      "Loss: inf, Train accuracy: 0.133333, val accuracy: 0.200000\n",
      "Loss: inf, Train accuracy: 0.066667, val accuracy: 0.200000\n"
     ]
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **60%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be real number, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 22\u001b[0m\n\u001b[1;32m     15\u001b[0m val_history \u001b[39m=\u001b[39m []\n\u001b[1;32m     17\u001b[0m \u001b[39m# TODO find the best hyperparameters to train the network\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m# You should expect to get to at least 40% of valudation accuracy\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m# Save loss/train/history of the best classifier to the variables above\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39;49m\u001b[39mbest validation accuracy achieved: \u001b[39;49m\u001b[39m%f\u001b[39;49;00m\u001b[39m'\u001b[39;49m \u001b[39m%\u001b[39;49m best_val_accuracy)\n",
      "\u001b[0;31mTypeError\u001b[0m: must be real number, not NoneType"
     ]
    }
   ],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = 1e-4\n",
    "reg_strength = 1e-3\n",
    "learning_rate_decay = 0.999\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of valudation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
